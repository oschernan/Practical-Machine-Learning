{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Image Processing w/ CNN - Oscar Hernandez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are advising the management team of a website provider on which machine learning model works best for image classification. They are looking for advise on a tool that can be used to automatically label images that end users provide. This report will outline the work that went into building several binary classification models while using a 2x2 completely crossed experimental design. Specifically, we will be training/testing convolutional neural networks (CNNs) using Keras running on top of TensorFlow. We will be utilizing a subset of the Asirra data set which includes images of dogs and cats to develop our CNNs. Our main objective/concern is developing a classification model with the highest possible accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This report will be broken up into Sections that cover the specific methodology that went into arriving at the final recommendation. The report will include and/or cover, at minimum, the following specific items and/or tasks:\n",
    "* Complete a quick exploration of the dogs and cats data set \n",
    "* Partitioning of the data set into training and test sets (due to size of data set, we chose not to create a validation set)\n",
    "* Train four CNNs using identical hyperparameters except the number of layers and number of neurons within each layer\n",
    "* Check each CNN for accuracy using our training and test subset\n",
    "* Evaluate the differences (focus on accurracy) observed across treatment combinations  \n",
    "* Provide final management recommendation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Exploratory Data Analysis & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1 covers loading the data sets that will be used to train our CNNs. We will also create the y labels for our data which were not provided to us.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the necessary packages we need to complete the exercise\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import nbconvert\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the cat/dog data as NumPY arrays\n",
    "#We were provided two separate NumPY array files for cats and dogs\n",
    "cats_1000_64_64_1 = np.load('cats_dogs_64-128/cats_1000_64_64_1.npy')\n",
    "dogs_1000_64_64_1 = np.load('cats_dogs_64-128/dogs_1000_64_64_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 64, 64, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can infer from the output of shape attribute, that the cats data \n",
    "#is 1000 observations and each image is 64x64 pixels \n",
    "#This same info applies for the dogs data  \n",
    "cats_1000_64_64_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's combine the separate files into one NumPY array\n",
    "#The first line of code stacks the files on top of each other\n",
    "#along the column axis \n",
    "X_cat_dog = np.concatenate((cats_1000_64_64_1, dogs_1000_64_64_1), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 64, 64, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the shape just to be sure \n",
    "X_cat_dog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we need to create our y labels using the concatenate function \n",
    "#where the first 1000 observaations are 0, which are the cat labels\n",
    "#and the last 1000 observations are 1, which are the dog labels\n",
    "#In our binary model, the 1 label means the observation is a dog\n",
    "#and the 0 label means the observation is a cat \n",
    "y_cat_dog = np.concatenate((np.zeros((1000), dtype = np.int32), \n",
    "                      np.ones((1000), dtype = np.int32)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, lets use the Scikit-Learn function to split both of \n",
    "#the 2D NumPY arrays into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cat_dog, y_cat_dog,\n",
    "                                                   test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Key Takeaways \n",
    "* In this section, we started by loading the cat and dog data which were stored as numpy arrays\n",
    "* We checked the shape of the arrays which is necessary since it is an input into our CNN later on\n",
    "* We combined both arrays into one and then also created y labels \n",
    "* The final step was creating a training and test data set by using a 80/20 split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Build Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2 will cover the steps conducted to create four CNNs. The only difference between each CNN is the number of layers and neurons per layer. This was done as part of the experiment to see the impact the change in these factors has on the accuracy of each CNN. \n",
    "\n",
    "#### We will be using Keras to build out each CNN. Also, we will be recording the time it takes to develop each one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classifier 1 Specs\n",
    "* Two convolution layers\n",
    "* 32 nodes per layer\n",
    "* ReLU activation function except for the output layer (Sigmoid)\n",
    "* Kernel size = 3 (3x3 filter matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 25s 16ms/step - loss: 7.9698 - acc: 0.5006\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 26s 16ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 25s 16ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 26s 16ms/step - loss: 8.0389 - acc: 0.5013\n"
     ]
    }
   ],
   "source": [
    "#Each model will be built in the following way: \n",
    "#Create initial model object using Keras Sequential function\n",
    "#Use the add method to create each layer \n",
    "#Compile the model by choosing the optimizer and cost function \n",
    "#Train the model using the cat/dog data set \n",
    "cnn1_start_time =time.time()\n",
    "\n",
    "cnn1 = Sequential()\n",
    "cnn1.add(Conv2D(32, kernel_size = 3, activation =\"relu\", input_shape=(64,64,1)))\n",
    "cnn1.add(Conv2D(32, kernel_size =3, activation =\"relu\"))\n",
    "#Connection between convolution and dense layer\n",
    "cnn1.add(Flatten())\n",
    "cnn1.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "cnn1.compile(optimizer = \"rmsprop\",loss=\"binary_crossentropy\", \n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "cnn1.fit(X_train, y_train, epochs = 5, batch_size = 20)\n",
    "\n",
    "cnn1_elapsed_time = time.time() - cnn1_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.03 seconds\n"
     ]
    }
   ],
   "source": [
    "print (round(cnn1_elapsed_time,2), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classifier 2 Specs\n",
    "* Two convolution layers\n",
    "* 16 nodes per layer\n",
    "* ReLU activation function except for the output layer (Sigmoid)\n",
    "* Kernel size = 3 (3x3 filter matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 8.0427 - acc: 0.5000\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 8.0389 - acc: 0.5012\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 12s 8ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 8.0389 - acc: 0.5013\n"
     ]
    }
   ],
   "source": [
    "cnn2_start_time =time.time()\n",
    "\n",
    "cnn2 = Sequential()\n",
    "cnn2.add(Conv2D(16, kernel_size = 3, activation =\"relu\", input_shape=(64,64,1)))\n",
    "cnn2.add(Conv2D(16, kernel_size =3, activation =\"relu\"))\n",
    "#Connection between convolution and dense layer\n",
    "cnn2.add(Flatten())\n",
    "cnn2.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "cnn2.compile(optimizer = \"rmsprop\",loss=\"binary_crossentropy\", \n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "cnn2.fit(X_train, y_train, epochs = 5, batch_size = 20)\n",
    "\n",
    "cnn2_elapsed_time = time.time() - cnn2_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.84 seconds\n"
     ]
    }
   ],
   "source": [
    "print (round(cnn2_elapsed_time,2), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classifier 3 Specs\n",
    "* Four convolution layers\n",
    "* 32 nodes per layer\n",
    "* ReLU activation function except for the output layer (Sigmoid)\n",
    "* Kernel size = 3 (3x3 filter matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 62s 39ms/step - loss: 8.0430 - acc: 0.4963\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 61s 38ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 74s 46ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 84s 52ms/step - loss: 8.0389 - acc: 0.5013\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 92s 58ms/step - loss: 8.0389 - acc: 0.5013\n"
     ]
    }
   ],
   "source": [
    "cnn3_start_time =time.time()\n",
    "\n",
    "cnn3 = Sequential()\n",
    "cnn3.add(Conv2D(32, kernel_size = 3, activation =\"relu\", input_shape=(64,64,1)))\n",
    "cnn3.add(Conv2D(32, kernel_size =3, activation =\"relu\"))\n",
    "cnn3.add(Conv2D(32, kernel_size =3, activation =\"relu\"))\n",
    "cnn3.add(Conv2D(32, kernel_size =3, activation =\"relu\"))\n",
    "#Connection between convolution and dense layer\n",
    "cnn3.add(Flatten())\n",
    "cnn3.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "cnn3.compile(optimizer = \"rmsprop\",loss=\"binary_crossentropy\", \n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "cnn3.fit(X_train, y_train, epochs = 5, batch_size = 20)\n",
    "\n",
    "cnn3_elapsed_time = time.time() - cnn3_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373.81 seconds\n"
     ]
    }
   ],
   "source": [
    "print (round(cnn3_elapsed_time,2), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Classifier 4 Specs\n",
    "* Four convolution layers\n",
    "* 16 nodes per layer\n",
    "* ReLU activation function except for the output layer (Sigmoid)\n",
    "* Kernel size = 3 (3x3 filter matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 29s 18ms/step - loss: 7.9444 - acc: 0.4963\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 28s 17ms/step - loss: 7.9911 - acc: 0.4988\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 28s 17ms/step - loss: 7.9911 - acc: 0.4988\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 32s 20ms/step - loss: 7.9911 - acc: 0.4988\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 37s 23ms/step - loss: 7.9911 - acc: 0.4988\n"
     ]
    }
   ],
   "source": [
    "cnn4_start_time =time.time()\n",
    "\n",
    "cnn4 = Sequential()\n",
    "cnn4.add(Conv2D(16, kernel_size = 3, activation =\"relu\", input_shape=(64,64,1)))\n",
    "cnn4.add(Conv2D(16, kernel_size =3, activation =\"relu\"))\n",
    "cnn4.add(Conv2D(16, kernel_size =3, activation =\"relu\"))\n",
    "cnn4.add(Conv2D(16, kernel_size =3, activation =\"relu\"))\n",
    "#Connection between convolution and dense layer\n",
    "cnn4.add(Flatten())\n",
    "cnn4.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "cnn4.compile(optimizer = \"rmsprop\",loss=\"binary_crossentropy\", \n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "cnn4.fit(X_train, y_train, epochs = 5, batch_size = 20)\n",
    "\n",
    "cnn4_elapsed_time = time.time() - cnn4_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.48 seconds\n"
     ]
    }
   ],
   "source": [
    "print (round(cnn4_elapsed_time,2), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "* This section was where we trained each neural network using the training data set \n",
    "* Specifically, we trained four different CNNs\n",
    "* The only difference between each CNN was the # of convolution layers and the # of nodes within each layer \n",
    "* Classifier 1: 2 layers and 32 nodes\n",
    "* Classifier 2: 2 layers and 16 nodes\n",
    "* Classifier 3: 4 layers and 32 nodes\n",
    "* Classifier 4: 4 layers and 16 nodes \n",
    "* We noted that adding additional layers significantly increased the development time \n",
    "* Final observations will be communicated after model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4 - Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section will calculate the classification accuracy for the training and test sets for each model. We will apply the evaluate method on each model and use the training or test sets with the same batch size as parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 3s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "#Classifier 1\n",
    "#Evaluate model on test data set\n",
    "score_test1 = cnn1.evaluate(X_test, y_test, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 9s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model on training data set \n",
    "score_train1 = cnn1.evaluate(X_train, y_train, batch_size =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.038900166749954, 0.5012500012293458] [8.139638304710388, 0.49500000178813935]\n"
     ]
    }
   ],
   "source": [
    "#This prints out the loss value and accuracy, respectively \n",
    "#Printing out the result from training and test set, respectively \n",
    "print (score_train1, score_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 5s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#Classifier 2\n",
    "#Evaluate model on training data set\n",
    "score_train2 = cnn2.evaluate(X_train, y_train, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model on test data set \n",
    "score_test2 = cnn2.evaluate(X_test, y_test, batch_size =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.038900166749954, 0.5012500012293458] [8.139638304710388, 0.49500000178813935]\n"
     ]
    }
   ],
   "source": [
    "#This prints out the loss value and accuracy, respectively \n",
    "#Printing out the result from training and test set, respectively \n",
    "print (score_train2, score_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 23s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "#Classifier 3\n",
    "#Evaluate model on training data set\n",
    "score_train3 = cnn3.evaluate(X_train, y_train, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 5s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model on test data set \n",
    "score_test3 = cnn3.evaluate(X_test, y_test, batch_size =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.038900166749954, 0.5012500012293458] [8.139638304710388, 0.49500000178813935]\n"
     ]
    }
   ],
   "source": [
    "#This prints out the loss value and accuracy, respectively \n",
    "#Printing out the result from training and test set, respectively \n",
    "print (score_train3, score_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 12s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "#Classifier 4\n",
    "#Evaluate model on training data set\n",
    "score_train4 = cnn4.evaluate(X_train, y_train, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 3s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model on test data set \n",
    "score_test4 = cnn4.evaluate(X_test, y_test, batch_size =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.991120874881744, 0.49874999914318324] [7.89148097038269, 0.505000002682209]\n"
     ]
    }
   ],
   "source": [
    "#This prints out the loss value and accuracy, respectively \n",
    "#Printing out the result from training and test set, respectively \n",
    "print (score_train4, score_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "* The results from this exercise were disappointing \n",
    "* All of the models had relatively the same accuracy performance on the test set -- approximately 50%, which is the equivalent to a coin flip in terms of predicting whether an image is a dog or cat\n",
    "* The model with the best performance was Classifier 4, which had an accuracy score of 50.5% on the test set \n",
    "* In terms of studying the impact of the benchmark experiment, there really isn't much to conclude since the results were very similar. In other words, there was no observable difference between the different factor inputs of layers and nodes/layer \n",
    "* If development time were important, we would choose Classifier 2 because its accuracy score was 49.5% but it had the shortest time \n",
    "* Overall, further experimentation is required to improve the accuracy scores. As noted, hyperparameters play an important part in developing a CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](HW7.jpg \"Table1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5 - Benchmark Experiment Results and Final Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on past research, we know that convolutional neural networks work best for image classification, speech recognition and natural language processing. Therefore, our final recommendation would be for the website provider to utilize a convolutional nueral network to help them classify images provided by their users. Unfortunately, the results of this experiment do not provide evidence for such a recommendation given the poor performance as measured by binary classification accuracy. Reflecting back on our results, we did not use multiple connected layers or make any augmentations to the data that was provided. In other words, further hyperparameter tuning seems to be an option that should be explored to improve the accuracy of these models. Therefore, because of the results, we cannot make an appropriate recommendation on which type of CNN to use. As mentioned, the lack of data augmentation may have played a part in our results. It would seem that using images with a larger shape (greater than 64 x 64) and on the RGB scale work best for this data set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
